{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f551fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e059dc",
   "metadata": {},
   "source": [
    "# To-Do:\n",
    "- Drop severerisk\n",
    "- Drop snow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab029865",
   "metadata": {},
   "source": [
    "# Thoughts, Questions, and Considerations:\n",
    "- We don't necessarily want to include pandemic-era data, as that is known to be anomalous.\n",
    "- Things like major storms (e.g. Harvey) may be worth removing as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5068813a",
   "metadata": {},
   "source": [
    "# Joining DataFrames and Creating Unified CSV:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297c9a1",
   "metadata": {},
   "source": [
    "## Houston:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf42818",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'houston_00_04.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dk/lzs3plw14ms00nxw2vwq68vc0000gn/T/ipykernel_19072/993781677.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhstn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'houston_00_04.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhstn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'houston_05_09.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhstn3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'houston_10_14.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhstn4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'houston_15_19.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhstn5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'houston_20_22.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'houston_00_04.csv'"
     ]
    }
   ],
   "source": [
    "hstn1 = pd.read_csv('houston_00_04.csv')\n",
    "hstn2 = pd.read_csv('houston_05_09.csv')\n",
    "hstn3 = pd.read_csv('houston_10_14.csv')\n",
    "hstn4 = pd.read_csv('houston_15_19.csv')\n",
    "hstn5 = pd.read_csv('houston_20_22.csv')\n",
    "houston = pd.concat([hstn1, hstn2, hstn3, hstn4, hstn5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec60c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "houston.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50864d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "houston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095b18a",
   "metadata": {},
   "source": [
    "## Galveston:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gvsn1 = pd.read_csv('galveston_00_04.csv')\n",
    "gvsn2 = pd.read_csv('galveston_05_09.csv')\n",
    "gvsn3 = pd.read_csv('galveston_10_14.csv')\n",
    "gvsn4 = pd.read_csv('galveston_15_19.csv')\n",
    "gvsn5 = pd.read_csv('galveston_20_22.csv')\n",
    "galveston = pd.concat([gvsn1, gvsn2, gvsn3, gvsn4, gvsn5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd9d48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "galveston.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c5afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "galveston.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191b5a7",
   "metadata": {},
   "source": [
    "## Port Lavaca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed5a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptlv1 = pd.read_csv('portlavaca_00_04.csv')\n",
    "ptlv2 = pd.read_csv('portlavaca_05_09.csv')\n",
    "ptlv3 = pd.read_csv('portlavaca_10_14.csv')\n",
    "ptlv4 = pd.read_csv('portlavaca_15_19.csv')\n",
    "ptlv5 = pd.read_csv('portlavaca_20_22.csv')\n",
    "port_lavaca = pd.concat([ptlv1, ptlv2, ptlv3, ptlv4, ptlv5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981648a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_lavaca.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_lavaca.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914bbd32",
   "metadata": {},
   "source": [
    "## Victoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca07bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vctr1 = pd.read_csv('victoria_00_04.csv')\n",
    "vctr2 = pd.read_csv('victoria_05_09.csv')\n",
    "vctr3 = pd.read_csv('victoria_10_14.csv')\n",
    "vctr4 = pd.read_csv('victoria_15_19.csv')\n",
    "vctr5 = pd.read_csv('victoria_20_22.csv')\n",
    "victoria = pd.concat([vctr1, vctr2, vctr3, vctr4, vctr5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5074b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "victoria.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a348ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "victoria.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f263d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "houston.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17060a3",
   "metadata": {},
   "source": [
    "# Defining Functions to Clean DataFrames for Concatenation:\n",
    "- Steps Needed:\n",
    "    - Take in df\n",
    "    - Dropping Columns function:\n",
    "        - Drop non-weather columns, like address, lat/long, etc. \n",
    "        - Also snow, snowdepth, and severerisk, source, preciptype, precipprob, conditions (others, if needed)\n",
    "    - Datetime function:\n",
    "        - Convert 'datetime' column to datetime format\n",
    "        - Set index to datetime\n",
    "        - Drop dates > 12-31-09 and < 7-1-17.\n",
    "        - Resample by the hour to clean up the data a bit    \n",
    "    - Rename all relevant columns to contain location name (separate function, most likely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9029f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "houston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ec489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = houston.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0935f",
   "metadata": {},
   "source": [
    "## Dropping Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05037cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drp_cols = ['name', 'address', 'resolvedAddress', 'latitude', 'longitude', 'snow', 'snowdepth', 'severerisk','preciptype','precipprob','source','conditions']\n",
    "# df.drop(columns = drp_cols, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ec83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#houston = drop_columns(houston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b408469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#houston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b8c6c",
   "metadata": {},
   "source": [
    "## Converting to Datetime and Setting Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_cleaner(df):\n",
    "    '''\n",
    "    Takes in a DataFrame, converts date column to DateTime format,\n",
    "    sets the index to the DateTime column, filters dates to include\n",
    "    2010 - June 2017, and resamples by the hour. \n",
    "    \n",
    "    Returns: pandas.DataFrame\n",
    "    Parameters: \n",
    "        df: pandas.DataFrame\n",
    "    '''\n",
    "    #converting 'datetime' column to datetime format:\n",
    "    df.datetime = pd.to_datetime(df.datetime)\n",
    "    #setting index to datetime:\n",
    "    df = df.set_index('datetime')\n",
    "    #filter dates:\n",
    "    df = df[(df.index > '12-31-2009') & (df.index < '7-1-2017')]\n",
    "    #resampling by hour:\n",
    "    df = df.resample('h').mean()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f89d9d",
   "metadata": {},
   "source": [
    "## Renaming Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11959dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df, city):\n",
    "    '''\n",
    "    Takes in a DataFrame and a string of the city name, \n",
    "    return a pandas DataFrame with all the columns renamed\n",
    "    to include the city name. \n",
    "    '''\n",
    "    for col in df.columns:\n",
    "        df.rename(columns = {col: (city + '_' + col)}, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812cdcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93077b6a",
   "metadata": {},
   "source": [
    "## Total Function, Step-Wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_cleaner(df, city):\n",
    "    '''\n",
    "    Takes in a DataFrame and a city name, drops unneeded columns, \n",
    "    cleans up the 'datetime' column and places it as the index, \n",
    "    and renames all measurement columns to include their city of \n",
    "    observation.\n",
    "    '''\n",
    "    #Dropping unneeded columns:\n",
    "    drp_cols = ['name', 'address', 'resolvedAddress', 'latitude', 'longitude', 'snow', 'snowdepth', 'severerisk','preciptype','precipprob','source','conditions']\n",
    "    df.drop(columns = drp_cols, inplace = True)\n",
    "    #Cleaning up dates:\n",
    "    df = date_cleaner(df)\n",
    "    #Renaming Columns:\n",
    "    df = rename_columns(df, city)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359074dd",
   "metadata": {},
   "source": [
    "# Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [houston, galveston, port_lavaca, victoria]\n",
    "city_list = ['hs', 'gv', 'pl', 'vc']\n",
    "total_df = pd.DataFrame()\n",
    "\n",
    "for df, city in zip(df_list, city_list):\n",
    "    city_df = weather_cleaner(df, city)\n",
    "    total_df = pd.concat([total_df, city_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4952e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.to_csv('total_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60360fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# To-Do:\n",
    "- Drop severerisk\n",
    "- Drop snow\n",
    "\n",
    "# Thoughts, Questions, and Considerations:\n",
    "- We don't necessarily want to include pandemic-era data, as that is known to be anomalous.\n",
    "- Things like major storms (e.g. Harvey) may be worth removing as well\n",
    "\n",
    "# Joining DataFrames and Creating Unified CSV:\n",
    "\n",
    "## Houston:\n",
    "\n",
    "hstn1 = pd.read_csv('houston_00_04.csv')\n",
    "hstn2 = pd.read_csv('houston_05_09.csv')\n",
    "hstn3 = pd.read_csv('houston_10_14.csv')\n",
    "hstn4 = pd.read_csv('houston_15_19.csv')\n",
    "hstn5 = pd.read_csv('houston_20_22.csv')\n",
    "houston = pd.concat([hstn1, hstn2, hstn3, hstn4, hstn5])\n",
    "\n",
    "houston.shape[0]\n",
    "\n",
    "houston.head()\n",
    "\n",
    "## Galveston:\n",
    "\n",
    "gvsn1 = pd.read_csv('galveston_00_04.csv')\n",
    "gvsn2 = pd.read_csv('galveston_05_09.csv')\n",
    "gvsn3 = pd.read_csv('galveston_10_14.csv')\n",
    "gvsn4 = pd.read_csv('galveston_15_19.csv')\n",
    "gvsn5 = pd.read_csv('galveston_20_22.csv')\n",
    "galveston = pd.concat([gvsn1, gvsn2, gvsn3, gvsn4, gvsn5])\n",
    "\n",
    "galveston.shape[0]\n",
    "\n",
    "galveston.head(2)\n",
    "\n",
    "## Port Lavaca:\n",
    "\n",
    "ptlv1 = pd.read_csv('portlavaca_00_04.csv')\n",
    "ptlv2 = pd.read_csv('portlavaca_05_09.csv')\n",
    "ptlv3 = pd.read_csv('portlavaca_10_14.csv')\n",
    "ptlv4 = pd.read_csv('portlavaca_15_19.csv')\n",
    "ptlv5 = pd.read_csv('portlavaca_20_22.csv')\n",
    "port_lavaca = pd.concat([ptlv1, ptlv2, ptlv3, ptlv4, ptlv5])\n",
    "\n",
    "port_lavaca.shape[0]\n",
    "\n",
    "port_lavaca.head(2)\n",
    "\n",
    "## Victoria:\n",
    "\n",
    "vctr1 = pd.read_csv('victoria_00_04.csv')\n",
    "vctr2 = pd.read_csv('victoria_05_09.csv')\n",
    "vctr3 = pd.read_csv('victoria_10_14.csv')\n",
    "vctr4 = pd.read_csv('victoria_15_19.csv')\n",
    "vctr5 = pd.read_csv('victoria_20_22.csv')\n",
    "victoria = pd.concat([vctr1, vctr2, vctr3, vctr4, vctr5])\n",
    "\n",
    "victoria.shape[0]\n",
    "\n",
    "victoria.head(2)\n",
    "\n",
    "houston.dtypes\n",
    "\n",
    "# Defining Functions to Clean DataFrames for Concatenation:\n",
    "- Steps Needed:\n",
    "    - Take in df\n",
    "    - Dropping Columns function:\n",
    "        - Drop non-weather columns, like address, lat/long, etc. \n",
    "        - Also snow, snowdepth, and severerisk, source, preciptype, precipprob, conditions (others, if needed)\n",
    "    - Datetime function:\n",
    "        - Convert 'datetime' column to datetime format\n",
    "        - Set index to datetime\n",
    "        - Drop dates > 12-31-09 and < 7-1-17.\n",
    "        - Resample by the hour to clean up the data a bit    \n",
    "    - Rename all relevant columns to contain location name (separate function, most likely)\n",
    "\n",
    "houston\n",
    "\n",
    "df = houston.copy()\n",
    "\n",
    "## Dropping Columns:\n",
    "\n",
    "# drp_cols = ['name', 'address', 'resolvedAddress', 'latitude', 'longitude', 'snow', 'snowdepth', 'severerisk','preciptype','precipprob','source','conditions']\n",
    "# df.drop(columns = drp_cols, inplace = True)\n",
    "\n",
    "#houston = drop_columns(houston)\n",
    "\n",
    "#houston.head()\n",
    "\n",
    "## Converting to Datetime and Setting Index:\n",
    "\n",
    "def date_cleaner(df):\n",
    "    '''\n",
    "    Takes in a DataFrame, converts date column to DateTime format,\n",
    "    sets the index to the DateTime column, filters dates to include\n",
    "    2010 - June 2017, and resamples by the hour. \n",
    "    \n",
    "    Returns: pandas.DataFrame\n",
    "    Parameters: \n",
    "        df: pandas.DataFrame\n",
    "    '''\n",
    "    #converting 'datetime' column to datetime format:\n",
    "    df.datetime = pd.to_datetime(df.datetime)\n",
    "    #setting index to datetime:\n",
    "    df = df.set_index('datetime')\n",
    "    #filter dates:\n",
    "    df = df[(df.index > '12-31-2009') & (df.index < '7-1-2017')]\n",
    "    #resampling by hour:\n",
    "    df = df.resample('h').mean()\n",
    "    return df\n",
    "\n",
    "## Renaming Columns:\n",
    "\n",
    "def rename_columns(df, city):\n",
    "    '''\n",
    "    Takes in a DataFrame and a string of the city name, \n",
    "    return a pandas DataFrame with all the columns renamed\n",
    "    to include the city name. \n",
    "    '''\n",
    "    for col in df.columns:\n",
    "        df.rename(columns = {col: (city + '_' + col)}, inplace = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df.head(1)\n",
    "\n",
    "## Total Function, Step-Wise:\n",
    "\n",
    "def weather_cleaner(df, city):\n",
    "    '''\n",
    "    Takes in a DataFrame and a city name, drops unneeded columns, \n",
    "    cleans up the 'datetime' column and places it as the index, \n",
    "    and renames all measurement columns to include their city of \n",
    "    observation.\n",
    "    '''\n",
    "    #Dropping unneeded columns:\n",
    "    drp_cols = ['name', 'address', 'resolvedAddress', 'latitude', 'longitude', 'snow', 'snowdepth', 'severerisk','preciptype','precipprob','source','conditions']\n",
    "    df.drop(columns = drp_cols, inplace = True)\n",
    "    #Cleaning up dates:\n",
    "    df = date_cleaner(df)\n",
    "    #Renaming Columns:\n",
    "    df = rename_columns(df, city)\n",
    "    return df\n",
    "\n",
    "# Loop:\n",
    "\n",
    "df_list = [houston, galveston, port_lavaca, victoria]\n",
    "city_list = ['hs', 'gv', 'pl', 'vc']\n",
    "total_df = pd.DataFrame()\n",
    "\n",
    "for df, city in zip(df_list, city_list):\n",
    "    city_df = weather_cleaner(df, city)\n",
    "    total_df = pd.concat([total_df, city_df], axis = 1)\n",
    "\n",
    "total_df.to_csv('total_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
